---
title: "Selection of goal-consistent acoustic environments by adults and children"
subtitle: "Anonymous CogSci Submission"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
abstract: >
    Children are navigating a world with massive amounts of auditory input, sometimes relevant while other times purely noise, and must somehow make sense of it all. The early auditory environment is critical for speech perception and recognition, auditory discrimination, and word learning, all of which support language outcomes. What strategies do children use to learn in noisy environments? One potential strategy is environmental selection, which allows children to seek environments that align with particular goals. In the current paper, we examined whether children and adults make decisions about their environments by integrating auditory information and goal-states. While 3- and 4-year olds struggle with discrmininating the level of noise in noisy speech streams (and likely do not use this information for environmental selection), 5-year-old children and adults can. Further, we show initial evidence that they can use this information to reason about acoustic environments that are consistent with specific goals. 
    
keywords: >
    active learning; auditory discrimination; auditory noise; cognitive development  
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=TRUE, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(ggthemes)
library(janitor)
library(brms)
library(rstanarm)
library(here)
```

# Introduction

Children engage with their auditory environment from birth, and these experiences support  speech perception and language development. With relevant stimuli, however, also comes noise. Noise is both ubiquitous and unavoidable, from sounds as low as a whisper (30dB) to as high as crowded restaurants (90dB) [@erickson2017]. Children struggle with speech perception and word recognition in noisy environments, and often require signal-to-noise (SNR) levels of 5-7dB higher than adults listening to the same stimulus [@bjorklund1990; @klatte2013]. Despite this, children manage to make sense of such a noisy world. 

More than 20 million children living in the United States are exposed to dangerous noise levels daily, and 5 million of those children suffer from noise-induced hearing loss as a result [@viet2014]. Unfortunately, children of color living in urban regions are overrepresented in these numbers [@casey2017]. Chronic exposure to noise has been correlated with poorer reading performance, reduced short term and episodic memory, and smaller expressive vocabularies in elementary school children [@clark20123; @hygge2019; @riley2012].Yet despite suboptimal conditions, language acquisition, cognitive development, and full engagement with the environment is still possible, albeit more difficult. What strategies do children use in these conditions? 

One observation is that children's attention or discrimination abilities may shift when faced with suboptimal auditory patterns, even if this causes deleterious long- term outcomes. @cohen1973 measured the sound pressure levels in and around a noisy Manhattan high-rise apartment complex where 8- and 9-year-old middle class students lived. Auditory discrimination mediated the relationship between reading comprehension/ability and auditory noise. Children exposed to higher levels of auditory noise in the home filtered out noise, but appeared to lose important information in the process.

Children might also learn to optimize their auditory environments to successfully complete certain goals. For example, a child might find that reading is best done in a library, not just because of its convention (because libraries function as places to read/check out books), but because it is a quiet space. Such a strategy might allow children to exploit environmental variation in noise to maximize their ability to learn in suboptimal or variable conditions. In the current paper, we asked whether preschool children can reason about their auditory environment and how it relates to specific goals.

Environmental selection of this type is a type of active learning, in which an agent makes choices to shape its own learning. The dominant approach to studying active learning has emphasized how learners approach individual stimuli [e.g., @settles2009]. When faced with uncertainty, both human and machine systems can learn actively by choosing new stimuli to query that are informative with respect to the learner’s current knowledge state [@castro2008]. Infants, too, have been shown to use active learning strategies [@ruggeri2019; see @xu2019 for review]. 

Although most active learning research has focused on stimulus selection, perhaps children and adults are engaging in active learning by also making decisions about the environments in which they learn. In practice, this behavior may present itself as moving to a different room to study for an upcoming exam or playing in a room with other children who seem to be having the kind of fun you desire. We might expect humans to seek out environments that best support their goals, and observe this strategy even in young children. 

In the current paper, we took a first step towards investigating whether children and adults actively select their auditory environment to achieve their goals. We conducted two experiments with both children and adults. Although our primary interest is whether and how children engage in environmental selection, we also collected adult samples to offer comparisons of how cognitively mature individuals might respond to these tasks. To ensure that the stimuli we use can be discriminated by children in our target ages, Experiments 1a and 1b investigate children and adults’ auditory discrimination of noise in long speech streams. Experiments 2a and 2b then examine whether children and adults can select auditory environments that match a goal.

# Experiment 1a

Previous research has consistently shown that adults can discriminate when two different sounds are at or below 5db apart, and children as young as four perform similarly to adults in discriminating contrasts as low as 5db [@jensen1993]. However, the stimuli commonly used to measure auditory (or intensity) discrimination tend to be short tonal bursts. These differ considerably from children’s real-world auditory experiences, which are not always transient and can reflect more sustained noise. Additionally, noise exposure is not limited to non-speech noise (e.g., white noise).  Thus, in our first experiment, we aimed to build on previous discrimination studies by creating a noise discrimination and preference paradigm that used longer audio streams (up to 25s) and naturalistic multi-talker noise. 

## Methods

```{r e1-data}
## Load data
data_1a <- read_csv(here("data","adultdatalog1a.csv")) #load data
answers_1a <- read_csv(here("data","adultanswerlog1a.csv")) %>%
  clean_names()#load answers

## Clean data
cleandata_1a <- data_1a %>% 
  clean_names() %>% #lowercase column names
  select(q6:q4) %>% #remove unnecessary columns
  rename("sound_check" = "q6",
         "debrief" = "q39",
         "loudness_type" = "q49",
         "age" = "q1",
         "gender" = "q3",
         "race" = "q2",
         "english" = "q4",
         "25" = "ij_test",
         "15" = "gh_test",
         "20" = "ef_test",
         "5" = "cd_test",
         "10" = "ab_test")  #rename all columns to something I understand 

## Remove first two rows (contain no data/are unnecessary)
cleandata_1a <- cleandata_1a[-c(1,2), ]

## Tidy data
tidydata_1a <- cleandata_1a %>% 
  filter(attention_check_1 == "Madras Paneer",
         sound_check == "Yes") %>% #exclude participants who failed attention and sound checks
  mutate(subject_id = row_number()) %>% #make a subject id column
  select(subject_id, everything()) %>%  #put subject id column first
  pivot_longer(cols = c("5", "10", "15", "20", "25"),
               names_to = "snr",
               values_to = "response") %>% #move all SNR values to a single column
  mutate(snr = as.numeric(snr),
         response = as.numeric(response)) #make SNR values and participant responses numeric for calculation

## Join tidydata_1a with answer log
joineddata_1a <- left_join(tidydata_1a, answers_1a, by = "snr")

## Measure performance 
completedata_1a <- joineddata_1a %>% 
  group_by(snr) %>% 
  mutate(correct = case_when(response == answer ~ 1,
                             response != answer ~ 0)) %>%  #add a correct column 
  na.omit() #remove any NA values

## Get demographic data
completedata_1a$age <- as.integer(completedata_1a$age) #age

demodata_1a <- completedata_1a %>%
  ungroup(snr) %>% 
  select(subject_id, race, age, snr) %>%
  filter(snr == 5) 
```

### Participants

A total of `r nrow(demodata_1a)` adults (mean age = `r round(mean(demodata_1a$age),2)` years; `r round(mean(demodata_1a$race == "Caucasian/White")*100,1)`% Caucasian/White) living in the United States at the time of test were recruited to participate via the online platform Prolific. Testing was restricted to a laptop, desktop, or tablet. All participants were fluent in English and had no severe visual or cognitive impairments. Informed consent was collected from each participant before the experiment began.

### Materials and Procedure

```{r e1-stimuli, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, fig.cap = "One of 10 animated classrooms participants viewed during the session."}
figure1 <- png::readPNG(here("writeup","figs","figure1.png"))
grid::grid.raster(figure1)
```

Participants were told that they would watch 25-second animated videos from each of the ten classrooms in The Alphabet School, a fictional preschool program in which each class learns one letter of the alphabet from A-J. Classrooms were created with Vyond animation software. Each classroom was depicted in the videos as having 5--6 preschool children and one adult teacher with stereotypical male or female presentation. The wall colors of each classroom identified which classroom participants were viewing. In each video, the teacher would tell the students which letter of the alphabet they would be learning, followed by three images on a whiteboard of animals or objects that begin with that letter. Figure \@ref(fig:e1-stimuli) illustrates one of the ten classrooms shown during the session.

Participants viewed two videos per trial, for a total of five trials.
Importantly, the classrooms differed in their signal-to-noise ratios (SNR), which ranged from 5--25dB. Each teacher’s speech was registered at 65dB, and the background noise, a recording of live preschool classrooms collected by the first author, were equalized on speech subtracting any silence in the clips, and ranged from 35--60dB. The two videos for each trial differed from each other in noise level by 5--25dB. The At the end of each trial, participants indicated which classroom was the louder of the two. To ensure that participants understood the referent of the question, we also asked at the end of the experiment whether the term “louder” referred to the loudness of the speaker or the loudness of the background noise. Additionally, to reduce participant inattention in the data, we included two attention check questions and excluded participants who answered at least one question incorrectly. SNR levels of each classroom were counterbalanced across trials and conditions.

## Results and Discussion

```{r e1a-bar, fig.pos = "t", fig.align='center', fig.cap="Results from Experiment 1a. Proportion of correct responses across SNR levels from 5--25dB. Error bars show 95\\% confidence intervals."}
## Calculate confidence intervals
cidata_1a <- completedata_1a %>%
  group_by(snr) %>%
  summarise(ci.l = binom::binom.bayes(x = sum(correct), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(correct), n = n())$upper,
            n = n(),
            mean_correct = mean(correct))

## Data visualization and stats

### Figure 2
ggplot(data = cidata_1a, mapping = aes(x = snr, y = mean_correct, fill = snr)) +
  geom_col() +
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("SNR") +
  ylab("Proportion of Correct Responses") +
  geom_hline(yintercept = .5, lty =2) + 
  viridis::scale_fill_viridis() + 
  theme_few() +
  theme(legend.position = "none") 
```

```{r e1-stats, echo=FALSE, results="hide"}
completedata_1a$snr_centered <- scale(completedata_1a$snr, scale = FALSE)

glmer_1a <- stan_glmer(formula = correct ~ snr_centered + (snr_centered | subject_id),
                       family = binomial,
                       data = completedata_1a)

summary_1a <- summary(glmer_1a, probs = c(0.025, 0.975))

```

Given prior data, we expected that across SNR levels, adults would correctly identify relative differences in the auditory environments presented in this experiment (which served primarily as a comparison for Experiment 1b with children). We preregistered a Bayesian mixed-effects logistic regression predicting correct responding as a function of SNR, with a maximal random effect structure (random slopes by SNR and a random intercept by participant). SNR level was centered at 15 dB. In this and subsequent models, we used default weakly informative priors (normal distributions on coefficients with SD=2.5, scaled to predictor magnitudes).

On average, adults were above chance across all five SNR levels (intercept: $\beta$ = `r round(summary_1a[1,][["mean"]],2)`, 95% Crl = [`r round(summary_1a[1,][["2.5%"]],2)` - `r round(summary_1a[1,][["97.5%"]],2)`]), and there was a modest effect of SNR on performance (intercept: $\beta$ = `r round(summary_1a[2,][["mean"]],2)`, 95% Crl = [`r round(summary_1a[2,][["2.5%"]],2)` - `r round(summary_1a[2,][["97.5%"]],2)`]). Data are shown in Figure \@ref(fig:e1a-bar).

This finding is both a replication of previous studies which have found similar performance levels in adults, as well as an extension that revealed these findings hold even with more complex stimuli. These results affirm adults' auditory discrimination skills are fully mature, and that they possess the cognitive resources necessary to successfully complete this task. 

# Experiment 1b

In Experiment 1b, we reran the same experiment with 3--5-year-old-children. 

## Methods

```{r e1b-data}
## Load data
data_1b <- read_csv(here("data", "childrendatalog1b.csv")) #load data
answers_1b <- read_csv(here("data", "childrenanswerlog1b.csv")) %>%  #load answers
  clean_names()

## Clean data
cleandata_1b <- data_1b %>% 
  clean_names() #lowercase column names

## Tidy data
tidydata_1b <- cleandata_1b %>% 
  filter(arm == "Main",
         included == "Yes") #exclude pilot participants and those who could not be retained for analysis failed attention and sound checks

## Join tidydata_1b with answer log
joineddata_1b <- left_join(tidydata_1b, answers_1b, by = c("condition", "trial"))

## Scale SNR and Age
joineddata_1b$snr_centered <- scale(joineddata_1b$snr, scale = FALSE) #snr
joineddata_1b$age_centered <- scale(joineddata_1b$age_in_months, scale = FALSE) #age

## Measure performance
completedata_1b <- joineddata_1b %>% 
  group_by(snr) %>% 
  mutate(correct = case_when(response == answer ~ 1,
                             response != answer ~ 0))  #add a correct column 
# na.omit() #remove any NA values

## Get demographic data
completedata_1b$age <- as.integer(completedata_1b$age_in_years) #age

demodata_1b <- completedata_1b %>%
  ungroup(snr) %>% 
  select(subject_id,race, age_in_years, snr) %>%
  filter(snr == 5)
```

### Participants

`r nrow(demodata_1b)` children (mean age = `r round(mean(demodata_1b$age_in_years), 2)` years, `r round(mean(demodata_1b$race == "Caucasian/White")*100,1)`% Caucasian/White) completed the same task as adults in Experiment 1a with a few notable differences. An additional `r nrow(cleandata_1b$included == "No")/5` children were ultimately excluded from analysis because their caregivers indicated they heard English less than 75% of the time.

### Materials and Procedure

Children were tested synchronously over the Zoom platform by an undergraduate research assistant. The researcher first collected informed consent from the caregiver, who was often present but instructed not to engage during the session, followed by assent from the child. Children whose caregivers pointed to the computer screen or provided answers during the session were excluded from analysis. Due to the age range of interest, the experiment was presented strictly though images and videos, and the research assistant verbally explained each slide to the children. Between trials, children were rewarded with virtual gold stars, which also served to pace the experiment and to maintain engagement. Finally, children were not asked to identify the referent of the question. 

## Results and Discussion

```{r e1b-bar, fig.env = "figure", fig.pos = "t", fig.align='center', fig.cap = "Experiment 1b. Proportion of correct responses across SNR levels from 5--25dB. Error bars show 95\\% confidence intervals."}

cidata_1b <- completedata_1b %>%
  group_by(snr, age) %>%
  summarise(ci.l = binom::binom.bayes(x = sum(correct), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(correct), n = n())$upper,
            n = n(),
            mean_correct = mean(correct))

### Figure 3
ggplot(data = cidata_1b, 
       mapping = aes(x = snr, y = mean_correct, fill = snr)) +
  geom_col() +
  facet_wrap(~age) + 
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("SNR") +
  ylab("Proportion of Correct Responses") +
  geom_hline(yintercept = .5, lty =2) + 
  viridis::scale_fill_viridis(guide=FALSE) + 
  theme_few() 
```


```{r e1b-stats, results="hide"}
### Logistic regression
glmer_1b <- stan_glmer(formula = correct ~ age_in_months * snr_centered + (snr_centered | subject_id),
                       family = binomial,
                       data = completedata_1b)

summary_1b <- summary(glmer_1b, probs = c(0.025, 0.975))

glmer_1b_years <- stan_glmer(formula = correct ~ age_in_years * snr_centered + (snr_centered | subject_id),
                       family = binomial,
                       data = completedata_1b)

summary_1b <- summary(glmer_1b, probs = c(0.025, 0.975))
summary_1b_years <- summary(glmer_1b_years, probs = c(0.025, 0.975))

```

We anticipated that, while the strength of the effect would increase with age, all children would correctly identify relative differences in SNRs from 10--25dB, and that only three-year-old children would be unable to correctly identify this difference at 5dB. We ran the same Bayesian logistic regression presented in Experiment 1a, but added age (centered at the mean) as a main effect. Figure \@ref(fig:e1b-bar) demonstrates a similar, though weaker, pattern of auditory discrimination skills in preschool children. In the aggregate, 3-5-year-old children showed some discrimination ability on the current paradigm (intercept : $\beta$ = `r round(summary_1b[1,][["mean"]],2)`, Crl = [`r round(summary_1b[1,][["2.5%"]],2)` - `r round(summary_1b[1,][["97.5%"]],2)`]), but independent of SNR (intercept : $\beta$ = `r round(summary_1b[3,][["mean"]],2)`, Crl = [`r round(summary_1b[3,][["2.5%"]],2)` - `r round(summary_1b[3,][["97.5%"]],2)`]).

Age played a larger role in children's performance than we anticipated. To explore this effect, we binned the data by the child's age in years [3;0-3;11, 4;0-4;11, and 5;0-5;11 years] and reran the same analysis. Figure 4 illustrates this age effect:  older children were more likely to correctly discriminate auditory signals than younger children (intercept : $\beta$ = `r round(summary_1b_years[1,][["mean"]],2)`, Crl = [`r round(summary_1b_years[1,][["2.5%"]],2)` - `r round(summary_1b_years[1,][["97.5%"]],2)`]).

Our findings differed from prior results in that only 5 year olds appeared to be robustly above chance in discrimination. There are several possible reasons for this disparity. First, as described earlier, this task is much more challenging than prior rapid discrimination tasks: it requires assessing the level of noise in a video, remembering it, and comparing it to another over the courses of almost a minute.  Additionally, the type of stimuli presented here differs from the tonal bursts or other non-speech sounds used in earlier work. 

# Experiment 2a

If 5-year-old participants can successfully discriminate between sound pressure levels, can they then use this information to reason about which goals are most appropriate in these environments? In our next set of experiments, we assessed this hypothesis. Participants watched a video of a third-person character with several goals and were asked to select the environment in which he should complete these goals. As in Experiment 1, we began by assessing performance in a convenience sample of adults. 

## Methods

```{r e2a-data}
## Load data
data_2a <- read_csv(here("data","adultdatalog2a.csv")) #load data
answers_2a <- read_csv(here("data", "adultanswerlog2a.csv")) %>% #load answers
  clean_names()

## Clean data
cleandata_2a <- data_2a %>% 
  clean_names() %>% #lowercase column names
  select(q6:experimental_session_dance_do) %>% #remove unnecessary columns
  rename("sound_check" = "q6")  #rename column to something I understand

answers_2a$activity <- tolower(answers_2a$activity) #change activity column to lowercase
answers_2a$activity <- tolower(str_trim(answers_2a$activity)) #remove extra spaces
answers_2a$louder <- tolower(answers_2a$louder) #change louder column to lowercase
answers_2a$quieter <- tolower(answers_2a$quieter) #change quieter column to lowercase

## Remove first row (contains no data/are unnecessary)
cleandata_2a <- cleandata_2a[-c(1,2), ]

## Tidy data
tidydata_2a <- cleandata_2a %>% 
  filter(attention_check_1 == "Fish",
         attention_check_2 == "Ryan",
         sound_check == "Yes") %>% #exclude participants who failed attention and sound checks
  mutate(subject_id = row_number()) %>% #make a subject id column
  select(subject_id, everything()) %>% #put subject id column first
  select(-c(read_justification,
            build_justification,
            paint_justification,
            talk_justification,
            zerpie_justification,
            learn_justification,
            dance_justification,
            eat_justification,
            reflection,
            prolific_pid,
            fl_21_do)) %>%  #remove more unnecessary columns
  pivot_longer(cols = c(read, build, eat, talk, dance, paint, learn, zerpie),
               names_to = "activity",
               values_to = "response") %>%   #move all activities to a single column
  separate(col = experimental_session_build_do,
           into = c(NA, "build", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_read_do,
           into = c(NA, "read", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_paint_do,
           into = c(NA, "paint", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_learn_do,
           into = c(NA, "learn", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_zerpie_do,
           into = c(NA, "zerpie", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_talk_do,
           into = c(NA, "talk", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_dance_do,
           into = c(NA, "dance", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_eat_do,
           into = c(NA, "eat", NA, NA),
           sep = '[|]') %>% #extract SNR values from display order
  pivot_longer(cols = c(read, build, eat, talk, dance, paint, learn, zerpie),
               names_to = "activity_label",
               values_to = "snr") #extract SNR values into their own column

## Join tidydata_2a with answer log
tidydata_2a$response<- tolower(tidydata_2a$response) #change response column to lowercase

tidydata_2a$snr <- as.integer(tidydata_2a$snr) #make snr values in tidydata_2a integers

joineddata_2a <- left_join(tidydata_2a, answers_2a, by = c("snr", "activity"))

## Combine louder and quieter columns for analysis
completedata_2a <- joineddata_2a %>% 
  pivot_longer(cols = c("louder","quieter"),
               names_to = "volume",
               values_to = "value") %>%
  filter(response == value,
         activity == activity_label) %>% 
  mutate(quiet = ifelse(volume == "quieter", 1, 0),
         room = ifelse(value == "a", 1, 2)) %>%  #recode volume data
  na.omit() #remove any NA values

## Get demographic data
completedata_2a$age <- as.integer(completedata_2a$age) #age

demodata_2a <- completedata_2a %>%
  select(subject_id,race, age)

```

### Participants

`r nrow(demodata_2a)/8` adults (mean age = `r round(mean(demodata_2a$age), 2)` years; `r round(mean(demodata_2a$race == "Caucasian/White")*100,1)`% Caucasian/White) living in the United States at the time of test were recruited to participate via the online platform, Prolific. An additional `r nrow(c(cleandata_2a$attention_check_1 != "Fish", cleandata_2a$attention_check_2 != "Ryan"))/8` were excluded from analysis for failing one or more of the attention checks. Testing was restricted to a laptop, desktop, or tablet. All participants were fluent in English and had no severe visual or cognitive impairments. Informed consent was collected from each participant before the experiment began. 

### Materials and Procedure

Participants were introduced to a preschool-aged character named Ryan with eight goals to complete throughout the experiment- (1) to read a book, (2) to build a tower out of blocks, (3) to learn the letters of the alphabet, (4) to paint a picture, (5) to dance to his favorite music, (6) to learn a new language called Zerpie, (7) to talk to a friend, and (8) to eat lunch. All activities had relatively simple explanations with the exception of (6). For this trial, participants were told that Ryan’s new neighbor, Logan, speaks a rare language called Zerpie, a language he doesn’t speak. Ryan wants to learn Zerpie so he can communicate with Logan. For all activities, Ryan must complete each task in a closed room. Each goal was presented individually as a single trial for a total of eight trials. In each trial, participants watched a video in which Ryan stood in between two closed doors labeled “A” and “B”, respectively. Before the video began, participants were told to watch and listen carefully to decide in which of the two rooms Ryan should complete his goal.

As in Experiment 1, we manipulated the sound level of each room, but removed any classroom stimuli, including the teacher, and only depicted one child opening and standing in front of each door. As such, participants did not have access to any visual information about the room, and could only rely on auditory information, as well as any information provided by the character who opened the door. Each character’s voice was equalized to 65dB and, unlike in Experiment 1, all characters shared the same voice. All characters except Ryan were preschool girls but differed in appearance. The same background noise in Experiment 1 was used for the current experiment, and the difference in SNR between the two rooms was 5--25dB. During the video, each character would open their respective door beginning with Room A. The character in Room A always said, “You can [goal] in this room”, while the character in Room B always said, “Or you can [goal] in this room.” While the room on the left was always labeled “A” and the room on the right was always labeled “B”, the characters from and sound levels of each room, as well as goal order were counterbalanced across conditions. 

For each trial, participants were told which goal Ryan wanted to complete and were asked to select the room that he should complete his goal. After making a selection, they were then asked to briefly explain their choice. Responses for the quieter room (relative to the other and based on the actual sound pressure level) were given a 1, while responses for the louder room were given a 0.

## Results and Discussion

```{r e2a-bar, fig.env = "figure", fig.pos = "t", fig.align='center', fig.cap = "Experiment 2a. Proportion of participants selecting the quiet room based on activity, with activities sorted by response level."}

## Calculate confidence intervals
cidata_2a <- completedata_2a %>%
  group_by(activity) %>% 
  summarise(ci.l = binom::binom.bayes(x = sum(quiet), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(quiet), n = n())$upper,
            n = n(),
            mean_quiet = mean(quiet)) %>% 
  mutate(activity = fct_reorder(activity, mean_quiet, .desc = TRUE))

## Data visualization and stats

### Figure 5
ggplot(data = cidata_2a, mapping = aes(x = activity, y = mean_quiet, fill = activity)) +
  geom_col() +
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("Activity") +
  ylab("Proportion Choosing Quieter Room") +
  geom_hline(yintercept = .5, lty =2) + 
  scale_color_solarized() + 
  theme_few() +
  theme(legend.position = "none") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r e2a-stats, results="hide"}
### Logistic regression

glmer_2a <- stan_glmer(formula = quiet ~ activity + (activity | subject_id),
                       family = binomial,
                       data = completedata_2a)

summary_2a <- summary(glmer_2a, probs = c(0.025,0.975))
```

We expected that adults would select the quieter room when the goal was (1) to read a book, (2) to learn the new language called Zerpie, and (3) to learn the letters of the alphabet, but would be more likely to select the louder room when the goal was (1) to dance to his favorite music, (2) to talk to a friend, and (3) to build a tower out of blocks. Additionally, we expected participants to have no sound level preference for (1) eating lunch and (2) painting a picture. This is because the latter two activities might be more ambiguous in the degree to which a relatively quieter or louder environment supports or hinders the goal. 

As in Experiments 1a and 1b, we preregistered a Bayesian mixed-effects logistic regression predicting environmental preference as a function of activity type. Figure \@ref(fig:e2a-bar) depicts adult participants' preferences for quieter environments based on the chosen activity. Coefficients for the read, learn, Zerpie, paint, and dance activities all had 95% credible intervals that did not overlap with zero. Interestingly, only for the dance activity did adults choose the louder room more than 50% of the time, likely reflecting some ambivalence about whether someone might want to, e.g., eat in a loud room. 

In sum, these findings suggests adults can reason about the match between acoustic environments and activity goals. 

# Experiment 2b

<!-- We hypothesized that adults would both consistently discriminate even the most challenging auditory signals, and that children would show a developmental trajectory in both auditory discrimination and environmental selection. In other words, older children would perform similarly to adults on the auditory discrimination task and would show weaker, though similar performance on the environmental selection task. -->

In the next next study, we asked about whether children could also evaluate the match between acoustic environments and activity goals. Following the results of Experiment 1b, we conducted this experimnt with five year olds only. 

## Methods

```{r}
# Experiment 2b

## Load data
data_2b <- read_csv(here("data", "childrendatalog2b.csv")) #load data
answers_2b <- read_csv(here("data", "childrenanswerlog2b.csv")) %>%  #load answers
  clean_names()

## Clean data
cleandata_2b <- data_2b %>% 
  clean_names() #lowercase column names

## Tidy data
tidydata_2b <- cleandata_2b %>% 
  filter(arm == "Main",
         included == "Yes") #exclude pilot participants and those who could not be retained for analysis failed attention and sound checks

## Join tidydata_2b with answer log
joineddata_2b <- left_join(tidydata_2b, answers_2b, by = c("condition", "trial"))

## Scale Age
joineddata_2b$age_centered <- scale(joineddata_2b$age_months, scale = FALSE) #age

## Measure performance
completedata_2b <- joineddata_2b %>% 
  pivot_longer(cols = c("louder","quieter"),
               names_to = "volume",
               values_to = "room") %>%
  filter(response == room) %>% 
  mutate(quiet = ifelse(volume == "quieter", 1, 0))
# %>%  #recode volume data
# na.omit() #remove any NA values

demodata_2b <- completedata_2b %>%
  select(subject_id,race, age_months, activity) %>%
  filter(activity == "build")
```

### Participants

`r nrow(demodata_2b)` 5-year-old children (mean age = `r round(mean(demodata_2a$age_months), 2)` months, `r round(mean(demodata_2a$race == "Caucasian/White")*100,1)`%% Caucasian/White) completed a truncated version of Experiment 2a to both prevent testing fatigue and to maximize any response differences based on the presented goals. 

We tested children on the Zoom platform and also also recruited a small group of children at a local preschool. This in-person testing was conducted with both caregiver consent and participant assent. As with the online testing, participants were inclued only if they heard English at home at least 75% of the time and had no known cognitive, visual, or neurological impairments. 
<!-- Given the finding from Experiment 1b that 3- and 4-year-old children performed near chance in the auditory discrimination task with long speech streams, we chose to only test 5-year-old children for this portion of the study. -->

### Materials and Procedure

We tested children on the four activities with the widest differences observed in Experiment 2a- (1) to read a book, (2) to learn the letters of the alphabet, (3) to build a tower out of blocks, and (4) to dance to music, for a total of four trials. Additionally, participants in this experiment were only shown videos in which the two rooms had SNR differences of 25dB, to maximize our chance of observing an effect. 

Rooms and characters depicted in the videos remained consistent with Experiment 2a, with one exception: the room labels, “A” and “B”, were replaced with one black circle for Room 1 and two black circles for Room 2. This change was implemented after finding that several participants in the pilot study seemed to favor the letter A over B, and because these letter labels may interfere with responses when the goal is to learn the letters of the alphabet. Black circle labels, on the other hand, are more abstract and may reduce this bias. As done previously, the characters, sound pressure levels, and goal order were counterbalanced across conditions.

Whether testing online or in-person, participants were shown the same set of videos and a research assistant (for online testing) or the first author (for in-person testing) verbally explained each slide and video to participants. After watching each video, participants were asked to select the room Ryan should complete his goal and to briefly explain their response. As in Experiment 2a, responses for the quieter room (relative to the other and based on the actual sound pressure level) were given a 1, while responses for the louder room were given a 0. 

## Results and Discussion

```{r 2b-bar, fig.env = "figure", fig.pos = "t", fig.align='center', fig.cap = "Experiment 2b. Proportion of participants selecting the quieter room by activity."}

## Calculate confidence intervals
cidata_2b <- completedata_2b %>%
  group_by(activity) %>%
  summarise(ci.l = binom::binom.bayes(x = sum(quiet), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(quiet), n = n())$upper,
            n = n(),
            mean_quiet = mean(quiet)) %>% 
  mutate(activity = fct_reorder(activity, mean_quiet, .desc = TRUE))

ggplot(data = cidata_2b, mapping = aes(x = activity, y = mean_quiet, fill = activity)) +
  geom_col() +
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("Activity") +
  ylab("Proportion Choosing Quieter Room") +
  geom_hline(yintercept = .5, lty =2) + 
  scale_color_solarized() + 
  theme_few() +
  theme(legend.position = "none") 
```

```{r e2b-stats, results="hide"}
### Logistic regression
completedata_2b$activity <- factor(completedata_2b$activity, 
                                   levels = c("read","build","learn","dance"))

# completedata_2b$noisy_activity <- completedata_2b$activity %in% c("build","dance")

glmer_2b <- stan_glmer(formula = quiet ~ activity + (activity | subject_id),
                       family = binomial,
                       data = completedata_2b)

glmer_2b_hypothesis <- brm(formula = quiet ~ activity + 
                  (activity | subject_id),
                family = "bernoulli",
                data = completedata_2b, 
                save_all_pars = TRUE)

glmer_2b_null <- brm(formula = quiet ~ 1 + 
                       (activity | subject_id),
                     family = "bernoulli",
                     data = completedata_2b,
                     control = list(adapt_delta = .99), 
                     save_all_pars = TRUE)

bf_mod <- bayes_factor(glmer_2b_hypothesis, glmer_2b_null)


summary_2b <- summary(glmer_2b, probs = c(.025, .975))
```

We expected to see a similar, though weaker, response pattern as adult participants in Experiment 2a. Figure \@ref(fig:e2b-bar) depicts childrens' preferences for quieter environments based on the chosen activity. We ran the same logistic regression as in Experiment 2a. Children were more likely than chance to select the quieter room for book reading (which was set to the intercept: $\beta$ = `r round(summary_2b[1,][["mean"]],2)`, Crl = [`r round(summary_2b[1,][["2.5%"]],2)` - `r round(summary_2b[1,][["97.5%"]],2)`]), but credible intervals for the other activities overlapped zero, suggesting that they could not be individually differentiated from those for the read activity. Overall, children appeared to have a preference for the quieter room across activities.

As an exploratory analysis, we asked whether the inclusion of other activity predictors as a whole improved model fit by using bridge sampling to compare between models with and without activity as a predictor. This comparison revealed a bayes factor of `r round(bf_mod[1]$bf,2)` in favor of the activity model, suggesting that as a whole these predictors did substantially improve model fit and hence children showed some sensitivity to goal in their room selections, despite their bias for the quieter room.

# General Discussion

These set of experiments demonstrate both an important relationship between auditory discrimination and a strategy children and adults use to optimize their auditory environments and achieve goals. Experiment 1a and 1b found that both 5-year-old children and adults can sufficiently discriminate auditory stimuli in long speech streams at sound pressure levels as low as 5dB. On the other hand, 3- and 4-year-old children were unable to do so. The challenge we observed in 3- and 4-year-old children's performance may reveal a lack of cognitive skills necessary to complete the task. The fact that 5-year-old children were performing above chance, however, may indicate that these skills are rapidly maturing by the fifth year of life. Experiment 2a displayed adult's selectivity of auditory environments based on changing goals while Experiment 2b demonstrated the opposite; 5-year-olds were more likely to select the quieter room regardless of the stated goal.

The latter finding does not necessarily reveal a failure on children's end, but rather a useful heuristic they might employ when they are still learning about much of their world. Whereas most adults have the flexibility of basic linguistic and cognitive mastery to seek louder environments for certain tasks, children can reduce uncertainty by selecting quieter auditory environments and then focus their attention and cognitive energy on achieving them. Previous work has shown that humans (including children) have an intrinsic desire to reduce uncertainty [@gottlieb2013]. In the wild, children pursue ways to reduce uncertainty when faced with a possible reward [@feldstein1971] and search for additional information on a particular topic when their intuitive theories are less informative [@wang2021]. Thus, this experiment may offer another example of uncertainty reduction in children.

This set of experiments is not without its limitations. The paradigm used in Experiment 2a included only 8 goal-based activities (and Experiment 2b offered just a subset of 4), which may not reflect the distribution of activities that elicit preferences for environments of certain noise levels. This paradigm also relied on third-party scenarios where participants assisted someone else with achieving certain goals. It is still unknown whether children would make similar decisions if they themselves were given goals to complete. Additionally, there is a possibility that familiarity and bias influenced participants' environmental preferences. Future work should explore novel activities where participants cannot rely on their current knowledge about which auditory environments are most optimal for each activity. 

By understanding the strategies children use to learn in noisy auditory environments, we might offer better solutions for those exposed to chronic noise, thereby mitigating some of its negative effects. This is becoming more and more critical as cities become more populated (bringing construction with it) and auditory noise becomes even more unavoidable. Future studies will need to (1) explore the developmental trajectory of environmental selection such that we can identify when indiscriminately selecting the quieter environment is no longer beneficial, and (2) examine the boundaries of environmental selection by probing these questions with other goals and in other contexts (e.g. first-person goal-setting). Investigating how children learn in noise will ultimately bring us closer to understanding the complex wonders of childhood.

# Acknowledgements

Removed for anonymous submission.   

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
