---
title: "Selection of goal-consistent acoustic environments by adults and preschool-aged children"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: >
   \author{\bf Rondeline M. Williams (rondeline.williams@stanford.edu) \and Michael C. Frank (mcfrank@stanford.edu)
   \\ Department of Psychology, Stanford University, Stanford, CA 94305 USA}
   
abstract: >
  Children are navigating a world with massive amounts of auditory input, sometimes relevant while other times purely noise, and must somehow make sense of it all. The early auditory environment is critical for speech perception and recognition, auditory discrimination, and word learning, all of which support language outcomes. What strategies do children use to learn in noisy environments? One potential strategy is environmental selection, which allows children to seek environments that align with particular goals. In the current paper, we examined whether children and adults make decisions about their environments by integrating auditory information and goal-states. While 3- and 4-year olds struggle with discriminating the level of noise in noisy speech streams (and likely do not use this information for environmental selection), 5-year-old children and adults can. Further, we show initial evidence that they can use this information to reason about acoustic environments that are consistent with specific goals. 
    
keywords: >
  active learning; auditory discrimination; auditory noise; cognitive development  
    
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=FALSE, 
                      message=F, sanitize = T)
```

```{r libraries}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(ggthemes)
library(janitor)
library(brms)
library(rstanarm)
library(here)
library(stats)
```

# Introduction

Children's auditory environment supports language development, but this environment can also be noisy and chaotic. Acoustic noise is ubiquitous and unavoidable, from sounds as low as a whisper (30 dB) to as high as crowded restaurants (90 dB) [@erickson2017]. Children struggle with speech perception and word recognition in noisy environments, and often require signal-to-noise (SNR) levels of 5-7 dB higher than adults listening to the same stimulus [@bjorklund1990; @klatte2013]. Despite this, children manage to make sense of such a noisy world. 

More than 20 million children living in the United States are exposed to dangerous noise levels daily, and 5 million of those children suffer from noise-induced hearing loss as a result [@viet2014]. Unfortunately, children of color living in urban regions are over-represented in these numbers [@casey2017]. Chronic exposure to noise has been correlated with poorer reading performance, reduced short term and episodic memory, and smaller expressive vocabularies in elementary school children [@clark20123; @hygge2019; @riley2012]. Yet despite suboptimal conditions, language acquisition, cognitive development, and full engagement with the environment is still possible, albeit more difficult. What strategies do children use in these conditions? 

One observation is that children's attention or discrimination abilities may shift when faced with suboptimal auditory patterns, even if this causes deleterious long- term outcomes. For example, @cohen1973 measured the sound pressure levels in and around a noisy Manhattan high-rise apartment complex where 8- and 9-year-old middle class students lived, and then asked how this chronic noise exposure related to reading performance. Auditory discrimination mediated the relationship between reading comprehension/ability and auditory noise, such that children exposed to higher levels of auditory noise in the home not only filtered out the noise, but also filtered out important information that may support reading ability. Because children were indiscriminately filtering out both acoustic signal and noise, this strategy might be considered maladaptive over time- one that primarily affects children exposed to chronic noise. 

It is possible, however, that children can and do make use of adaptive strategies under acoustic constraints. Consider a problem space in which children learn to optimize their auditory environments to successfully complete certain goals. For example, a child might find that reading is best done in a library, not just because of its convention (because libraries function as places to read/check out books), but because it is a quiet space. Such a strategy might allow children to exploit environmental variation in noise to maximize their ability to learn in suboptimal or variable conditions. In the current paper, we asked whether preschool children can reason about their auditory environment and how it relates to specific goals.

Environmental selection of this type is a type of active learning, in which an agent makes choices to shape its own learning. The dominant approach to studying active learning has emphasized how learners approach individual stimuli [e.g., @settles2009]. When faced with uncertainty, both human and machine systems can learn actively by choosing new stimuli to query that are informative with respect to the learner’s current knowledge state [@castro2008]. Infants, too, have been shown to use active learning strategies [@ruggeri2019; see @xu2019 for review]. 

Although most active learning research has focused on stimulus selection, perhaps children and adults are engaging in active learning by also making decisions about the environments in which they learn. In practice, this behavior may present itself as moving to a different room to study for an upcoming exam or playing in a room with other children who seem to be having the kind of fun you desire. We might expect humans to seek out environments that best support their goals, and observe this strategy even in young children. 

In the current paper, we took a first step towards investigating whether children and adults actively select their auditory environment to achieve their goals. We conducted two experiments with both children and adults. Although our primary interest is whether and how children engage in environmental selection, we also collected adult samples to offer comparisons of how cognitively mature individuals might respond to these tasks. To ensure that the stimuli we use can be discriminated by children in our target ages, Experiments 1a and 1b investigate children and adults’ auditory discrimination of noise in long speech streams. Experiments 2a and 2b then examine whether children and adults can select auditory environments that match a goal.

# Experiment 1a

Previous research has consistently shown that adults can discriminate when two different sounds are at or below 5 dB apart, and children as young as four perform similarly to adults in discriminating contrasts as low as 5 dB [@jensen1993]. However, the stimuli commonly used to measure intensity discrimination tend to be short tonal bursts. These differ considerably from children’s real-world auditory experiences, which are not always transient and can reflect more sustained noise. Additionally, noise exposure is not limited to non-speech noise (e.g., white noise).  Multi-talker noise is one initial example of a kind of noise that occurs in children’s natural environments and that has been used across other studies as a more ecological noise stimulus [@mcmillan2016;@fallon2000]. Thus, in our first experiment, we aimed to build on previous discrimination studies by creating a intensity discrimination and preference paradigm that used longer audio streams (up to 25s) and naturalistic multi-talker noise. This experiment (and its counterpart with children, Experiment 1b) sets the stage for further experiments on environmental selection.

## Methods

```{r e1-data}
## Load data
data_1a <- read_csv(here("data","adultdatalog1a.csv")) #load data
answers_1a <- read_csv(here("data","adultanswerlog1a.csv")) %>%
  clean_names()#load answers

## Clean data
cleandata_1a <- data_1a %>% 
  clean_names() %>% #lowercase column names
  select(q6:q4) %>% #remove unnecessary columns
  rename("sound_check" = "q6",
         "debrief" = "q39",
         "loudness_type" = "q49",
         "age" = "q1",
         "gender" = "q3",
         "race" = "q2",
         "english" = "q4",
         "25" = "ij_test",
         "15" = "gh_test",
         "20" = "ef_test",
         "5" = "cd_test",
         "10" = "ab_test")   #rename all columns to something I understand 
  
## Remove first two rows (contain no data/are unnecessary)
cleandata_1a <- cleandata_1a[-c(1,2), ]

## Tidy data
tidydata_1a <- cleandata_1a %>% 
  filter(attention_check_1 == "Madras Paneer",
         sound_check == "Yes") %>% #exclude participants who failed attention and sound checks
  mutate(subject_id = row_number()) %>% #make a subject id column
  select(subject_id, everything()) %>%  #put subject id column first
  pivot_longer(cols = c("5", "10", "15", "20", "25"),
               names_to = "snr",
               values_to = "response") %>% #move all SNR values to a single column
  mutate(snr = as.numeric(snr),
         response = as.numeric(response)) #make SNR values and participant responses numeric for calculation

## Join tidydata_1a with answer log
joineddata_1a <- left_join(tidydata_1a, answers_1a, by = "snr")

## Measure performance 
completedata_1a <- joineddata_1a %>% 
  group_by(snr) %>% 
  mutate(correct = case_when(response == answer ~ 1,
                             response != answer ~ 0)) %>% #add a correct column 
  na.omit() #remove any NA values

## Get demographic data
completedata_1a$age <- as.integer(completedata_1a$age) #age

demodata_1a <- completedata_1a %>%
  ungroup(snr) %>% 
  select(subject_id, race, age, snr) %>%
  filter(snr == 5) 

```

### Participants

A total of `r nrow(demodata_1a)` adults (mean age = `r round(mean(demodata_1a$age),2)` years; `r round(mean(demodata_1a$race == "Caucasian/White")*100,1)`% Caucasian/White) living in the United States at the time of test were recruited to participate via the online platform Prolific. Testing was restricted to a laptop, desktop, or tablet. All participants were fluent in English and had no severe visual or cognitive impairments. To preserve the quality of the data, participants also completed two attention check questions and were excluded if they failed one or more of the attention checks. For this reason, an additional 6 participants were excluded from analysis. Informed consent was collected from each participant before the experiment began.

### Materials and Procedure

```{r e1-stimuli, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, fig.cap = "One of 10 animated classrooms participants viewed during the session."}
figure1 <- png::readPNG(here("writeup","figs","figure1.png"))
grid::grid.raster(figure1)
```

Participants were told that they would watch 25s animated videos from each of the ten classrooms in The Alphabet School, a fictional preschool program in which each class learns one letter of the alphabet from A--J. Classrooms were created with Vyond animation software. Each classroom was depicted in the videos as having 5--6 preschool children and one adult teacher with stereotypical male or female presentation. The wall colors of each classroom identified which classroom participants were viewing. In each video, the teacher would tell the students which letter of the alphabet they would be learning, followed by three images on a whiteboard of animals or objects that begin with that letter. Each room corresponds to one video. Figure 1 illustrates one of the ten classrooms shown during the session.

Participants viewed two videos per trial, for a total of five trials. Importantly, the classrooms differed in their signal-to-noise ratios (SNR), which ranged from 5--25 dB. The target signal, the teacher's speech, was registered at 65 dB, and the background noise, a recording of live preschool classrooms, was equalized on speech subtracting any silence in the clips and registered at 35, 40, 45, 50, 55, or 60 dB. The target signal and background noise clips were then combined to create five videos with SNRs of 5, 10, 15, 20, or 25 dB. The two videos participants viewed for each trial differed from each other by 5--25 dB SNR (e.g. For Trial 1, if Video/Room A has an SNR of 10 dB and Video/Room B has an SNR of 25 dB, then the difference in SNR for Trial 1 is 15 dB). At the end of each trial, participants were asked, "Which room was louder- Room (A) or Room (B)". To understand how participants evaluated the referent of the question, we also asked at the end of the experiment whether the term “louder” [in the question, "Which room was louder- Room (A) or Room (B)"] referred to the loudness of the speaker or the loudness of the background noise, but was not an exclusion criteria. The majority of participants -- 33/40 -- indicated the loudness of the background noise as the referent of the question. Additionally, to reduce participant inattention in the data, we included two attention check questions and excluded participants who answered at least one question incorrectly. SNR levels of each classroom were counterbalanced across trials and conditions. Because SNR is a relative measure, the relative intensity between stimuli was standardized across participants [@klingholz1987]. While we do recognize potential differences in absolute intensity between participants, this difference alone likely has no significant bearing on the results presented here.

## Results and Discussion

```{r e1a-bar, fig.pos = "t", fig.align='center', fig.cap= "Results from Experiment 1a. Proportion of responses correctly indicating the stimuli with the greatest sound pressure level. Participants were presented with a binary choice and had a 50\\% chance of correctly responding. SNR levels on the x-axis ranged from (left to right) 5, 10, 15, 20, and 25 dB. Error bars show 95\\% confidence intervals."}
## Calculate confidence intervals
cidata_1a <- completedata_1a %>%
  group_by(snr) %>%
  summarise(ci.l = binom::binom.bayes(x = sum(correct), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(correct), n = n())$upper,
            n = n(),
            mean_correct = mean(correct))

## Data visualization and stats

### Figure 2
ggplot(data = cidata_1a, mapping = aes(x = snr, y = mean_correct, fill = snr)) +
  geom_col() +
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("SNR") +
  ylab("Prop. Correct") +
  geom_hline(yintercept = .5, lty =2) + 
  viridis::scale_fill_viridis() + 
  theme_few() +
  theme(legend.position = "none") 
```

```{r e1-stats, echo=FALSE, results="hide", eval=FALSE}
completedata_1a$snr_centered <- scale(completedata_1a$snr, scale = FALSE)

glmer_1a <- stan_glmer(formula = correct ~ snr_centered + (snr_centered | subject_id),
                       family = binomial,
                       data = completedata_1a)

summary_1a <- summary(glmer_1a, probs = c(0.025, 0.975))

saveRDS(summary_1a, here("writeup/models","summary_1a.Rds"))

```

```{r e1-stats-load}
summary_1a <- readRDS(here("writeup/models","summary_1a.Rds"))
```
Given prior data, we expected that across SNR levels, adults would correctly identify relative differences in the auditory environments presented in this experiment (which served primarily as a comparison for Experiment 1b with children). We preregistered [https://osf.io/tqay9] a Bayesian mixed-effects logistic regression predicting correct responding as a function of SNR, with a maximal random effect structure (random slopes by SNR and a random intercept by participant). SNR level was centered at 15 dB. In this and subsequent models, we used the package default of weakly informative priors (normal distributions on coefficients with SD=2.5, scaled to predictor magnitudes).

On average, adults were above chance across all five SNR levels (intercept: $\beta$ = `r round(summary_1a[1,][["mean"]],2)`, 95% Crl = [`r round(summary_1a[1,][["2.5%"]],2)` - `r round(summary_1a[1,][["97.5%"]],2)`]), and there was a modest effect of SNR on performance (intercept: $\beta$ = `r round(summary_1a[2,][["mean"]],2)`, 95% Crl = [`r round(summary_1a[2,][["2.5%"]],2)` - `r round(summary_1a[2,][["97.5%"]],2)`]). Data are shown in Figure 2.

This finding is both a replication of previous studies which have found similar performance levels in adults, as well as an extension that revealed these findings hold even with more complex stimuli. These results affirm adults' auditory discrimination skills are fully mature, and that they possess the cognitive resources necessary to successfully complete this task. 

# Experiment 1b

In Experiment 1b, we reran the same experiment with 3--5-year-old-children. 

## Methods

```{r e1b-data}
## Load data
data_1b <- read_csv(here("data", "childrendatalog1b.csv")) #load data
answers_1b <- read_csv(here("data", "childrenanswerlog1b.csv")) %>%  #load answers
  clean_names()

## Clean data
cleandata_1b <- data_1b %>% 
  clean_names() #lowercase column names

## Tidy data
tidydata_1b <- cleandata_1b %>% 
  filter(arm == "Main",
         included == "Yes") #exclude pilot participants and those who could not be retained for analysis failed attention and sound checks

## Join tidydata_1b with answer log
joineddata_1b <- left_join(tidydata_1b, answers_1b, by = c("condition", "trial"))

## Scale SNR and Age
joineddata_1b$snr_centered <- scale(joineddata_1b$snr, scale = FALSE) #snr
joineddata_1b$age_centered <- scale(joineddata_1b$age_in_months, scale = FALSE) #age

## Measure performance
completedata_1b <- joineddata_1b %>% 
  group_by(snr) %>% 
  mutate(correct = case_when(response == answer ~ 1,
                             response != answer ~ 0))  #add a correct column 
# na.omit() #remove any NA values

## Get demographic data
completedata_1b$age <- as.integer(completedata_1b$age_in_years) #age

demodata_1b <- completedata_1b %>%
  ungroup(snr) %>% 
  select(subject_id,race, age_in_years, snr) %>%
  filter(snr == 5)
```

### Participants

`r nrow(demodata_1b)` children (3;0 years-5;11 years, mean age = `r round(mean(demodata_1b$age_in_years), 2)` years, 12 children per age group, `r round(mean(demodata_1b$race == "Caucasian/White")*100,1)`% Caucasian/White) completed the same task as adults in Experiment 1a with a few notable differences. An additional `r sum(cleandata_1b$included == "No")/5` children were ultimately excluded from analysis because their caregivers indicated they heard English less than 75% of the time. Participants were recruited through online advertisements on social media and through direct sign-ups on a multi-lab developmental research website.

### Materials and Procedure

Children were tested synchronously over the Zoom platform by an undergraduate research assistant. The researcher first collected informed consent from the caregiver, who was often present but instructed not to engage during the session, followed by assent from the child. Children whose caregivers pointed to the computer screen or provided answers during the session were excluded from analysis. Due to the age range of interest, the experiment was presented strictly though images and videos, and the research assistant verbally explained each slide to the children. Between trials, children were given virtual gold stars, which served to pace the experiment and to maintain engagement. Children were not provided any feedback on their performance. Unlike adults, children were not asked to identify whether the speaker or the background was the referent of "louder." 

## Results and Discussion

```{r e1b-bar, fig.env = "figure", fig.pos = "t", fig.align='center', fig.cap = "Experiment 1b. Proportion of correct responses across SNR levels from 5--25 dB. Error bars show 95\\% confidence intervals."}

cidata_1b <- completedata_1b %>%
  group_by(snr, age) %>%
  summarise(ci.l = binom::binom.bayes(x = sum(correct), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(correct), n = n())$upper,
            n = n(),
            mean_correct = mean(correct))

### Figure 3
ggplot(data = cidata_1b, 
       mapping = aes(x = snr, y = mean_correct, fill = snr)) +
  geom_col() +
  facet_wrap(~age) + 
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("SNR") +
  ylab("Prop. Correct") +
  geom_hline(yintercept = .5, lty =2) + 
  viridis::scale_fill_viridis(guide="none") + 
  theme_few() 
```

```{r e1b-stats, results="hide", eval=FALSE}
### Logistic regression
glmer_1b <- stan_glmer(formula = correct ~ age_in_months * snr_centered + (snr_centered | subject_id),
                       cores = 4,
                       family = binomial,
                       data = completedata_1b)

summary_1b <- summary(glmer_1b, probs = c(0.025, 0.975))

glmer_1b_years <- stan_glmer(formula = correct ~ age_in_years * snr_centered + (snr_centered | subject_id),
                             cores = 4,
                       family = binomial,
                       data = completedata_1b)

summary_1b <- summary(glmer_1b, probs = c(0.025, 0.975))
summary_1b_years <- summary(glmer_1b_years, probs = c(0.025, 0.975))

saveRDS(summary_1b, here("writeup/models","summary_1b.Rds"))
saveRDS(summary_1b_years, here("writeup/models","summary_1b_years.Rds"))

```

```{r e1b-stats-load}
summary_1b <- readRDS(here("writeup/models","summary_1b.Rds"))
summary_1b_years <- readRDS(here("writeup/models","summary_1b_years.Rds"))
```

We anticipated that, while the strength of the effect would increase with age, all children would correctly identify relative differences in SNRs from 10--25 dB, and that only three-year-old children would be unable to correctly identify this difference at 5 dB. We ran the same Bayesian logistic regression presented in Experiment 1a, but added age (centered at the mean) as a main effect. Figure 3 demonstrates a similar, though weaker, pattern of auditory discrimination skills in preschool children. In the aggregate, 3--5-year-old children showed some discrimination ability on the current paradigm (intercept : $\beta$ = `r round(summary_1b[1,][["mean"]],2)`, Crl = [`r round(summary_1b[1,][["2.5%"]],2)` - `r round(summary_1b[1,][["97.5%"]],2)`]), but independent of SNR (intercept : $\beta$ = `r round(summary_1b[3,][["mean"]],2)`, Crl = [`r round(summary_1b[3,][["2.5%"]],2)` - `r round(summary_1b[3,][["97.5%"]],2)`]).

Age played a larger role in children's performance than we anticipated. To explore this effect, we binned the data by the child's age in years [3;0-3;11, 4;0-4;11, and 5;0-5;11 years] and reran the same analysis. Older children were more likely to correctly discriminate auditory signals than younger children (intercept : $\beta$ = `r round(summary_1b_years[1,][["mean"]],2)`, Crl = [`r round(summary_1b_years[1,][["2.5%"]],2)` - `r round(summary_1b_years[1,][["97.5%"]],2)`]).

Our findings differed from prior results in that only 5 year olds appeared to be robustly above chance in discrimination. There are several possible reasons for this disparity. First, as described earlier, this task is much more challenging than prior rapid discrimination tasks: it requires assessing the level of noise in a video, remembering it, and comparing it to another over the course of almost a minute.  Additionally, the type of stimuli presented here differs from the tonal bursts or other non-speech sounds used in earlier work. 

# Experiment 2a

If 5-year-old participants can successfully discriminate between sound pressure levels, can they then use this information to reason about which goals are most appropriate in these environments? In our next set of experiments, we assessed this hypothesis. Participants watched a video of a third-person character with several goals and were asked to select the environment in which he should complete these goals. As in Experiment 1, we began by assessing performance in a convenience sample of adults. 

## Methods

```{r e2a-data}
## Load data
data_2a <- read_csv(here("data","adultdatalog2a.csv")) #load data
answers_2a <- read_csv(here("data", "adultanswerlog2a.csv")) %>% #load answers
  clean_names()

## Clean data
cleandata_2a <- data_2a %>% 
  clean_names() %>% #lowercase column names
  select(q6:experimental_session_dance_do) %>% #remove unnecessary columns
  rename("sound_check" = "q6")  #rename column to something I understand
  
answers_2a$activity <- tolower(answers_2a$activity) #change activity column to lowercase
answers_2a$activity <- tolower(str_trim(answers_2a$activity)) #remove extra spaces
answers_2a$louder <- tolower(answers_2a$louder) #change louder column to lowercase
answers_2a$quieter <- tolower(answers_2a$quieter) #change quieter column to lowercase

## Remove first row (contains no data/are unnecessary)
cleandata_2a <- cleandata_2a[-c(1,2), ]

## Tidy data
tidydata_2a <- cleandata_2a %>% 
  filter(attention_check_1 == "Fish",
         attention_check_2 == "Ryan",
         sound_check == "Yes") %>% #exclude participants who failed attention and sound checks
  mutate(subject_id = row_number()) %>% #make a subject id column
  select(subject_id, everything()) %>% #put subject id column first
  select(-c(read_justification,
            build_justification,
            paint_justification,
            talk_justification,
            zerpie_justification,
            learn_justification,
            dance_justification,
            eat_justification,
            reflection,
            prolific_pid,
            fl_21_do)) %>%  #remove more unnecessary columns
  pivot_longer(cols = c(read, build, eat, talk, dance, paint, learn, zerpie),
               names_to = "activity",
               values_to = "response") %>%   #move all activities to a single column
  separate(col = experimental_session_build_do,
           into = c(NA, "build", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_read_do,
           into = c(NA, "read", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_paint_do,
           into = c(NA, "paint", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_learn_do,
           into = c(NA, "learn", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_zerpie_do,
           into = c(NA, "zerpie", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_talk_do,
           into = c(NA, "talk", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_dance_do,
           into = c(NA, "dance", NA, NA),
           sep = '[|]') %>%
  separate(col = experimental_session_eat_do,
           into = c(NA, "eat", NA, NA),
           sep = '[|]') %>% #extract SNR values from display order
  pivot_longer(cols = c(read, build, eat, talk, dance, paint, learn, zerpie),
               names_to = "activity_label",
               values_to = "snr") #extract SNR values into their own column

## Join tidydata_2a with answer log
tidydata_2a$response<- tolower(tidydata_2a$response) #change response column to lowercase

tidydata_2a$snr <- as.integer(tidydata_2a$snr) #make snr values in tidydata_2a integers

joineddata_2a <- left_join(tidydata_2a, answers_2a, by = c("snr", "activity"))

## Combine louder and quieter columns for analysis
completedata_2a <- joineddata_2a %>% 
  pivot_longer(cols = c("louder","quieter"),
               names_to = "volume",
               values_to = "value") %>%
  filter(response == value,
         activity == activity_label) %>% 
  mutate(quiet = ifelse(volume == "quieter", 1, 0),
         room = ifelse(value == "a", 1, 2)) %>%  #recode volume data
  na.omit() #remove any NA values

## Get demographic data
completedata_2a$age <- as.integer(completedata_2a$age) #age

demodata_2a <- completedata_2a %>%
  select(subject_id,race, age)

```

### Participants

`r nrow(demodata_2a)/8` adults (mean age = `r round(mean(demodata_2a$age), 2)` years; `r round(mean(demodata_2a$race == "Caucasian/White")*100,1)`% Caucasian/White) living in the United States at the time of test were recruited to participate via the online platform, Prolific. An additional 19 participants were excluded from analysis for failing one or more of the attention checks. Testing was restricted to a laptop, desktop, or tablet. All participants were fluent in English and had no severe visual or cognitive impairments. Informed consent was collected from each participant before the experiment began. 

### Materials and Procedure

Participants were introduced to a preschool-aged character named Ryan with eight goals to complete throughout the experiment: (1) to read a book, (2) to build a tower out of blocks, (3) to learn the letters of the alphabet, (4) to paint a picture, (5) to dance to his favorite music, (6) to learn a new language called Zerpie, (7) to talk to a friend, and (8) to eat lunch. All activities had relatively simple explanations with the exception of (6). For this trial, participants were told that Ryan's new neighbor, Logan, speaks a rare language called Zerpie, a language he doesn’t speak. Ryan wants to learn Zerpie so he can communicate with Logan.

In each of the eight trials, participants watched a video in which Ryan stood in between two closed doors labeled "A" and "B", respectively. Before the video began, participants were told to watch and listen carefully to decide which of the two rooms Ryan should go to in order to complete his goal.

As in Experiment 1, we manipulated the sound level of each room, but removed any classroom stimuli, including the teacher, and only depicted one child opening and standing in front of each door. As such, participants did not have access to any visual information about the room, and could only rely on auditory information, as well as any information provided by the character who opened the door. Each character’s voice was equalized to 65 dB and, unlike in Experiment 1, all characters shared the same voice. All characters except Ryan were preschool girls but differed in appearance. The same background noise in Experiment 1 was used for the current experiment. For each trial, the difference in SNR between the two rooms was randomly selected to be either 5, 10, 15, 20, or 25 dB such that on average participants heard a range of smaller and larger intensity differences. 

During the video, each character would open their respective door beginning with Room A. The character in Room A always said, “You can [goal] in this room”, while the character in Room B always said, “Or you can [goal] in this room.” While the room on the left was always labeled “A” and the room on the right was always labeled “B”, the characters from and sound levels of each room, as well as goal order were counterbalanced across conditions. 

For each trial, participants were told which goal Ryan wanted to complete and were asked to select the room that he should complete his goal. After making a selection, they were then asked to briefly explain their choice. Responses for the quieter room (relative to the other and based on the actual sound pressure level) were given a 1, while responses for the louder room were given a 0.

## Results and Discussion

```{r e2a-bar, fig.env = "figure", fig.pos = "t", fig.align='center', fig.cap = "Experiment 2a. Proportion of participants selecting the quiet room based on activity, with activities sorted by response level."}

## Calculate confidence intervals
cidata_2a <- completedata_2a %>%
  group_by(activity) %>% 
  summarise(ci.l = binom::binom.bayes(x = sum(quiet), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(quiet), n = n())$upper,
            n = n(),
            mean_quiet = mean(quiet)) %>% 
  mutate(activity = fct_reorder(activity, mean_quiet, .desc = TRUE))

## Data visualization and stats

### Figure 5
ggplot(data = cidata_2a, mapping = aes(x = activity, y = mean_quiet, fill = activity)) +
  geom_col() +
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("Activity") +
  ylab("Prop. Quieter") +
  geom_hline(yintercept = .5, lty =2) + 
  scale_color_solarized() + 
  theme_few() +
  theme(legend.position = "none") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r e2a-stats, results="hide", eval=FALSE}
### Logistic regression

glmer_2a <- stan_glmer(formula = quiet ~ activity + (activity | subject_id),
                       family = binomial,
                       cores = 4, 
                       iter = 5000,
                       data = completedata_2a)

summary_2a <- summary(glmer_2a, probs = c(0.025,0.975))
saveRDS(summary_2a, here("writeup/models","summary_2a.Rds"))

```

```{r e2a-stats-load}
summary_2a <- readRDS(here("writeup/models","summary_2a.Rds"))
```

We expected that adults would select the quieter room when the goal was (1) to read a book, (2) to learn the new language called Zerpie, and (3) to learn the letters of the alphabet. We were uncertain but thought that some adults might be more likely to select the louder room when the goal was (1) to dance to his favorite music, (2) to talk to a friend, and (3) to build a tower out of blocks because these are more social activities and louder rooms might imply more people being present. Additionally, we expected participants to have no sound level preference for (1) eating lunch and (2) painting a picture because the goals are unconnected with the auditory environment. 

As in Experiments 1a and 1b, we preregistered [https://osf.io/hjqys] a Bayesian mixed-effects logistic regression predicting environmental preference as a function of activity type. Figure 4 depicts adult participants' preferences for quieter environments based on the chosen activity. Coefficients for the read, learn, Zerpie, paint, and dance activities all had 95% credible intervals that did not overlap with zero. Interestingly, only for the dance activity did adults choose the louder room more than 50% of the time, likely reflecting some ambivalence about whether someone might want to, e.g., eat in a loud room. 

In sum, these findings suggests adults can reason about the match between acoustic environments and activity goals. 

# Experiment 2b

<!-- We hypothesized that adults would both consistently discriminate even the most challenging auditory signals, and that children would show a developmental trajectory in both auditory discrimination and environmental selection. In other words, older children would perform similarly to adults on the auditory discrimination task and would show weaker, though similar performance on the environmental selection task. -->

In the next next study, we asked about whether children could also evaluate the match between acoustic environments and activity goals. Following the results of Experiment 1b, we conducted this experiment exclusively with 5-year-olds. 

## Methods

```{r}
# Experiment 2b

## Load data
data_2b <- read_csv(here("data", "childrendatalog2b.csv")) #load data
answers_2b <- read_csv(here("data", "childrenanswerlog2b.csv")) %>%  #load answers
  clean_names()

## Clean data
cleandata_2b <- data_2b %>% 
  clean_names() #lowercase column names

## Tidy data
tidydata_2b <- cleandata_2b %>% 
  filter(arm == "Main",
         included == "Yes") #exclude pilot participants and those who could not be retained for analysis failed attention and sound checks

## Join tidydata_2b with answer log
joineddata_2b <- left_join(tidydata_2b, answers_2b, by = c("condition", "trial"))

## Scale Age
joineddata_2b$age_centered <- scale(joineddata_2b$age_months, scale = FALSE) #age

## Measure performance
completedata_2b <- joineddata_2b %>% 
  pivot_longer(cols = c("louder","quieter"),
               names_to = "volume",
               values_to = "room") %>%
  filter(response == room) %>% 
  mutate(quiet = ifelse(volume == "quieter", 1, 0))
# %>%  #recode volume data
# na.omit() #remove any NA values

demodata_2b <- completedata_2b %>%
  select(subject_id,race, age_months, activity) %>%
  filter(activity == "build")
```

### Participants

`r nrow(demodata_2b)` 5-year-old children (`r round(mean(demodata_2b$race == "Caucasian/White")*100,1)`% Caucasian/White) completed a truncated version of Experiment 2a to both prevent testing fatigue and to maximize any response differences based on the presented goals. 

Participants were initially recruited and tested at a local Bay Area preschool but due to COVID restrictions, testing moved exclusively online. In total, 8 participants were tested in-person and 22 were tested online. The in-person testing was conducted with both caregiver consent and participant assent. As with the online testing, participants were included only if they heard English at home at least 75% of the time and had no known cognitive, visual, or neurological impairments, which led to an exclusion of an additional `r sum(cleandata_2b$included == "No")/4` children.
<!-- Given the finding from Experiment 1b that 3- and 4-year-old children performed near chance in the auditory discrimination task with long speech streams, we chose to only test 5-year-old children for this portion of the study. -->

### Materials and Procedure

We tested children on the four activities with the widest differences observed in Experiment 2a: (1) to read a book, (2) to learn the letters of the alphabet, (3) to build a tower out of blocks, and (4) to dance to music, for a total of four trials. Additionally, participants in this experiment were only shown videos in which the two rooms had SNR differences of 25 dB because there were no differences in performance across SNR levels in Experiment 1b. 

Rooms and characters depicted in the videos remained consistent with Experiment 2a, with one exception: the room labels, "A" and "B", were replaced with one black circle for Room 1 and two black circles for Room 2. This change was implemented after finding that several participants in the pilot study seemed to favor the letter A over B, and because these letter labels may interfere with responses when the goal is to learn the letters of the alphabet. Black circle labels, on the other hand, are more abstract and may reduce this bias. As done previously, the characters, sound pressure levels, and goal order were counterbalanced across conditions.

Whether testing online or in-person, participants were shown the same set of videos and a research assistant (for online testing) or the first author (for in-person testing) verbally explained each slide and video to participants. After watching each video, participants were asked to select the room Ryan should complete his goal and to briefly explain their response. As in Experiment 2a, responses for the quieter room (relative to the other and based on the actual sound pressure level) were given a 1, while responses for the louder room were given a 0. 

## Results and Discussion

```{r 2b-bar, fig.env = "figure", fig.pos = "t", fig.align='center', fig.cap = "Experiment 2b. Proportion of participants selecting the quieter room by activity.",}

## Calculate confidence intervals
cidata_2b <- completedata_2b %>%
  group_by(activity) %>%
  summarise(ci.l = binom::binom.bayes(x = sum(quiet), n = n())$lower,
            ci.u = binom::binom.bayes(x = sum(quiet), n = n())$upper,
            n = n(),
            mean_quiet = mean(quiet)) %>% 
  mutate(activity = fct_reorder(activity, mean_quiet, .desc = TRUE))

ggplot(data = cidata_2b, mapping = aes(x = activity, y = mean_quiet, fill = activity)) +
  geom_col() +
  geom_linerange(aes(ymin = ci.l,
                     ymax = ci.u)) +
  ylim(0,1) +
  xlab("Activity") +
  ylab("Prop. Quieter") +
  geom_hline(yintercept = .5, lty =2) + 
  scale_color_solarized() + 
  theme_few() +
  theme(legend.position = "none") 
```

```{r e2b-stats, results="hide", eval=FALSE}
### Logistic regression
completedata_2b$activity <- factor(completedata_2b$activity, 
                                   levels = c("read","build","learn","dance"))

# completedata_2b$noisy_activity <- completedata_2b$activity %in% c("build","dance")

glmer_2b <- stan_glmer(formula = quiet ~ activity + (activity | subject_id),
                       family = binomial,
                       cores = 4, 
                       data = completedata_2b)

glmer_2b_hypothesis <- brm(formula = quiet ~ activity + 
                  (activity | subject_id),
                family = "bernoulli",
                data = completedata_2b, 
                cores = 4,
                iter = 10000,
                control = list(adapt_delta = .99), 
                save_all_pars = TRUE)

glmer_2b_null <- brm(formula = quiet ~ 1 + 
                       (activity | subject_id),
                     family = "bernoulli",
                     data = completedata_2b,
                     cores = 4,
                     iter = 10000,
                     control = list(adapt_delta = .99), 
                     save_all_pars = TRUE)

bf_mod <- bayes_factor(glmer_2b_hypothesis, glmer_2b_null)

summary_2b <- summary(glmer_2b, probs = c(.025, .975))

saveRDS(summary_2b, here("writeup/models","summary_2b.Rds"))
saveRDS(bf_mod, here("writeup/models","bf_2b.Rds"))
```

```{r e2b-stats-load}
summary_2b <- readRDS(here("writeup/models","summary_2b.Rds"))
bf_mod <- readRDS(here("writeup/models","bf_2b.Rds"))
```

We expected to see a similar, though weaker, response pattern as adult participants in Experiment 2a. Figure 5 depicts children's preferences for quieter environments based on the chosen activity. We ran the same logistic regression as in Experiment 2a. Children were more likely than chance to select the quieter room for book reading (which was set to the intercept: $\beta$ = `r round(summary_2b[1,][["mean"]],2)`, Crl = [`r round(summary_2b[1,][["2.5%"]],2)` - `r round(summary_2b[1,][["97.5%"]],2)`]), but credible intervals for the other activities overlapped zero, suggesting that they could not individually be differentiated from those for the read activity. Overall, children appeared to have a preference for the quieter room across activities. 

Children's preference across activities appeared different from those of adults. For example, adults strongly preferred to learn in a quiet room while children had numerically the lowest quiet preference for the learning activity. We speculate that children's associations with these activities may differ from those of adults: for example, many children may think of learning as something to be done in a noisy classroom setting. 

As an exploratory analysis, we asked whether the inclusion of activity predictors as a whole improved model fit over an intercept-only model by using bridge sampling to compare between models with and without activity as a predictor. This comparison revealed a Bayes Factor of `r round(bf_mod[1]$bf,2)` in favor of the activity model, suggesting that as a whole these predictors did substantially improve model fit and hence children showed some sensitivity to goal in their room selections, despite their bias for the quieter room. 

# General Discussion

We asked here whether adults and children can reason about how acoustic noise changes their environment. We found that both 5-year-old children and adults could discriminate noise levels differing by 5 dB in long-form auditory stimuli. On the other hand, 3- and 4-year-old children were unable to do so. We then asked whether 5-year-olds and adults would reason about which acoustic environments best matched a particular activity goal. Adults showed clear and graded sensitivity, choosing quieter environments for reading and learning and louder environments for dancing. Five-year-olds were more likely to select the quieter room overall but showed initial evidence that they differentiated between activities as well.

In other research, children in the age ranges we studied show evidence that they learn actively [@ruggeri2019;@xu2019], pursue ways to reduce uncertainty when faced with a possible reward [@feldstein1971], and search for additional information on a particular topic when their intuitive theories are less informative [@wang2021]. Yet we found that younger children struggled even to differentiate environments with different levels of noise, and even 5-year-olds showed only modest sensitivity to the congruence between acoustic environments and goals. Each of these tasks may have been challenging for children for reasons unrelated to their sensitivity to the underlying constructs, however. The discrimination task required encoding and comparing noise levels across two different 25s videos, which might have been challenging for reasons of attention and memory. And the environmental selection task required noticing that the rooms differed in noise levels and encoding their noise levels as well as associating different noise levels with particular activities. Thus, in future work we intend to explore simpler and more naturalistic paradigms for evaluating children's environmental selection abilities.

There are several further limitations that point the way towards new experiments. First, our research relied on convenience samples and so our specific estimates are not broadly generalizable to other populations. Second, the paradigm used third-party scenarios where participants assisted someone else with achieving certain goals; it is still unknown whether children would make similar decisions if they themselves were given goals to complete. Finally, there is a possibility that participants' familiarity with the context of particular activities (e.g., that they have typically danced in a noisy preschool classroom) influenced their environmental preferences. Future work should explore novel activities where participants cannot rely on their current knowledge about which auditory environments are most optimal for each activity. 
By understanding the strategies children use to learn in noisy auditory environments, we might offer better solutions for those exposed to chronic noise, thereby mitigating some of its negative effects. Such mitigation is becoming more and more critical as cities become more populated (bringing construction with it) and auditory noise becomes even more unavoidable. Future studies will need to (1) explore the developmental trajectory of environmental selection, and (2) examine the boundaries of environmental selection by probing these questions with other goals and in other contexts (e.g. first-person settings). Investigating how children learn in noise will ultimately bring us closer to understanding how children can thrive across a wide range of environments.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
